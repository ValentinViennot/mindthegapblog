[{"content":"Are you a fan of Wordle, the addictive word puzzle game? Have you ever wondered what is the best word to play first? In this blog post, I\u0026rsquo;ll explore the use of Convolutional Neural Networks (CNNs) to solve Wordle grids with a success rate of over 80%.\nI should clarify that I\u0026rsquo;m not a data scientist – while I have a passion for machine learning and have completed a few courses on the subject, I am not an AI expert. My understanding of the topic is limited, and I only decided to use CNNs for this experiment because that\u0026rsquo;s what I had learned in university. Please keep this in mind as you read through the post.\nWhat is the best word to play first in Wordle? As the game gained popularity a year ago, my partner and I found ourselves playing regularly. At some point, I noticed that we were both always starting our grids using the same first word every day. He was using \u0026ldquo;Reals\u0026rdquo; and I was using \u0026ldquo;Power\u0026rdquo;. To be fair I didn\u0026rsquo;t really think about it, just some kind of habit. But I realised he had better stats than me, maybe thanks to his choice.\nThere surely is a right, scientific answer to the question of what is the best word to play first in Wordle. One approach to finding this word could be to conduct a statistical analysis of the English language, considering factors such as the most common letters and their likelihood of appearing at certain positions in a 5-letter word. Boooooring.\nI remembered my Machine Learning classes and I thought that another, much more fun, way to go about it could be to teach a computer how to play Wordle, and observe. Maybe we could discover the secret to finding the best word to play first. There we go.\nProgramming Wordle for Machine Learning Computer programming is all about inputs and outputs (and one could argue that in an ideal world one would only use functional programming, but that will be another blog). What arguments do we pass to a function, and what results do we expect to see?\nFor our Wordle experiment, the output is clear: the daily Wordle word. But what about the input?\nModelising the game: mental model Note: There are many ways to approach this problem. My approach was influenced by my own experience playing the game and my understanding of CNNs. CNNs are particularly good at manipulating images, and my way to visualise the ongoing Wordle grid state in Wordle was a kind of image in my mind.\nOn The New York Times user interface, the input is a keyboard with some kind of state. It shows letters you can use, words you already tried, and for each letter if it was (1) not in the word, (2) in the word but wrongly placed, (3) in the word and rightfully placed. It also validates what words you’re allowed to play.\nTherefore, one way to represent this game state is as a matrix, with each position and letter representing the information we\u0026rsquo;ve learned about the word to be found. The matrix would have a 5x26 shape, on one axis the letter position (0-4) and on another axis the letter \u0026ldquo;code\u0026rdquo; (0-25). The value would tell us if the letter is (-2) not here for sure, (0) maybe here or nowhere, (1) somewhere in the word, or (2) here for sure.\nThis matrix is what I will adress as the \u0026ldquo;game state\u0026rdquo; in the following sections.\nFor each word we try, we can update the game state with the obtained information:\nGREEN: This letter is here (2), therefore no other letter can be here (-2). YELLOW: This letter might is somewhere (1), but we know for sure it’s not here (-2). NONE: This letter is nowhere in the word (-2). Using this mental model, we can code the logic of the game in Python. The \u0026ldquo;Wordle helper functions\u0026rdquo; and \u0026ldquo;Dataset generation\u0026rdquo; in the accompanying notebook contain all the necessary source code and steps for training a computer to play Wordle.\nNow that we have a way to represent the game state and a plan for programming the logic of the game, we can move on to building a Convolutional Neural Network (CNN) to solve Wordle grids.\nConvolutional Neural Networks: The Wordle Whisperers A Convolutional Neural Network (CNN) is a type of artificial neural network specifically designed for image recognition tasks. It is made up of layers of interconnected nodes, where each node represents a unit of computation. CNN classifiers, which is what we are going to build here, take a matrice of pixels as their input (the image), have multiple hidden computation layers, and their output layer is a 1D-array of the possible classes.\nIn the context of our Wordle experiment, we can think of each game state as an image, with the position and letter representing different pixels. By training a CNN on a large dataset of game states and their corresponding correct words, we can teach the model to recognize patterns in the game states and make predictions about the correct word to play.\nCNNs are particularly useful for our Wordle task as they are able to capture spatial relationships between pixels in an image. They not only consider one pixel but also its surroundings. In our case, not only one letter and its probabilities to be at this position, but also the surrounding letters and probabilities. This means that CNNs can learn to recognize patterns and features within the game states that might not be immediately apparent to a human observer.\nGenerating the dataset Before we can train our CNN to solve Wordle games, we need to have a dataset of possible game states and their corresponding correct words. In order to generate this dataset, we can use a simple player algorithm that iteratively plays random words from a dictionary, regardless of the game state. By saving the game state and the correct word for each iteration, we can build a large dataset of game states and their corresponding correct words.\nTo generate our dataset, we used a Wordle word list published on GitHub. This list contains 14855 words, therefore a random player has a 1/14855 probability of guessing the correct word on each turn. Given that the player can\u0026rsquo;t reuse a word, the probability of guessing the correct word on each subsequent turn decreases even further. But it\u0026rsquo;s so low anyway that we can consider it a binomial distribution (X∼Bin(n,p)) with a success probability of about 0.0001.\nUsing the cumulative binomial probabilities, we find that a random player has a chance of success (P(X\u0026gt;=1, n=6)) of about 0.06%. One game out of 1,600. By playing a large number of games, we can generate a dataset with a good balance of correct and incorrect guesses.\nIn order to generate a human-quality dataset, we only saved a progressed game state and the correct word for each game (LOOP_TRAINING, MATURE_TRAINING). This is because the initial random game state contains no information (only zeros) and would introduce only noise into our dataset.\nWe iterated on over 3.7 million random games, resulting in about 370,000 correct guesses.\nGenerating this random training data can take a significant amount of time. In the linked Google Colab notebook, I have saved the output of the cells so that you can look at the code without having to run the dataset generation yourself (took me about 25mn to generate all the data).\nCrafting the model Now that we have our dataset, it\u0026rsquo;s time to build the CNN model that will be used to solve Wordle games. In order to do this, we need to consider the shape of our input data (the game state) and the shape of our desired output (the correct word).\nThe input to our model is a matrix with 5 rows (corresponding to the 5 letters in the Wordle game) and 26 columns (corresponding to the 26 letters in the alphabet). The output of our model is a vector with 14855 elements, representing the \u0026ldquo;probabilities\u0026rdquo; of each word in our dictionary being the correct answer.\nOur CNN model will have the following architecture:\nThe CNN architecture for solving Wordle consists of a simple 5x26 game input state as the first layer, followed by a 2D convolution layer which allows neighboring elements to influence each other. This helps the CNN to learn what game states \u0026ldquo;look like\u0026rdquo; a word or another one, similar to how humans can recognize whether a sequence of letters looks like a probable English word. The output of the convolution layer is then flattened and passed through dense layers to reduce its size and extract relevant information for classification. Finally, the model diverges to a 14855-element array representing the possible words.\nTraining and evaluating the model Now that we\u0026rsquo;ve crafted our CNN model, it\u0026rsquo;s time to train it using the previously generated dataset. We\u0026rsquo;ll use Google Colab for this process and iterate over 15 epochs.\nWhile and after training the model, we\u0026rsquo;ll need to be able to measure how well it performs in the real Wordle. To evaluate the performance of our model, we will use two metrics:\nAccuracy: This is a common metric for evaluating machine learning models. It represents the rate of correct output guesses given a test input. However, it\u0026rsquo;s important to note that the accuracy of our model does not necessarily reflect its success at playing Wordle. This is because the input states used to calculate accuracy are already progressed (i.e., they are not completely random like when you start a Wordle game).\nWinning rate: To get a more accurate picture of our model\u0026rsquo;s performance, we will also calculate the \u0026ldquo;winning rate\u0026rdquo; of our model. To do this, we will simulate a large number of Wordle games (using a 6-try, cumulative game state feedback loop) and compute the rate of won games over the total number of played games.\nFor comparison, a completely random player has a winning rate of about 0.06% (calculated using cumulative binomial probabilities, P(X≥1, n=6)). For a more challenging and exciting benchmark, according to the New York Times, my personal winning rate is 96%.\nWordle Victory: CNNs Conquer the Game I put our CNN model to the test and the results were not quite bad! After just 15 minutes of training on a GPU runtime, the model was ready to take on the Wordle. Here\u0026rsquo;s what happened:\nGenerating the necessary data for training and testing took me 25 minutes The actual training process took just 15 minutes, with 15 epochs on a GPU! Model\u0026rsquo;s accuracy was a shy 79% – not so bad for a quick machine learning model! I put the model to the ultimate test by having it play 1000 random Wordle games. The winning rate? A pretty good 82% with just over 4 tries on average. While it may not be able to beat human players just yet, our CNN model is definitely on the right track!\nSo, What is The Best First Word to Play in Wordle? One question remains: what is the best first word to play in Wordle?\nThe first word played by our model is \u0026ldquo;Rales\u0026rdquo;. It appears to be accepted by Wordle, and to be an anagram of \u0026ldquo;Reals\u0026rdquo;, which was a favorite first move of my partner and seemed to yield higher winning rates. Could it be that the key to Wordle success lies in the word \u0026ldquo;Rales\u0026rdquo;? Our CNN model seems to think so.\nBONUS: Quordle? In the last cell of the Colab notebook, I have demoed using the trained CNN to play the Quordle variant of the Wordle game. As if playing Wordle wasn\u0026rsquo;t challenging enough, in Quordle, players must juggle four Wordle games at once.\nOne of the main limitations of CNNs is their inability to consider tradeoffs such as \u0026ldquo;explore vs exploit\u0026rdquo; when making decisions. In games like Wordle and Quordle, players often have to weigh the benefits of exploring their options (e.g. playing a word that may not be the correct one but provides more information) against the potential rewards of exploiting their current knowledge (e.g. playing a word that they believe is the correct one).\nThere is a cool article on this topic, using reinforcement learning to play Wordle.\nConclusion This experiment demonstrated the potential of Convolutional Neural Networks (CNNs) for solving Wordle grids and potentially even surpassing human players. By training a CNN model on a dataset of random Wordle games, I was able to achieve a success rate of 82% when tested on 1000 games.\nI hope that this blog post has provided an entertaining and informative look at the world of Wordle and machine learning. Whether you\u0026rsquo;re a seasoned Wordle pro or a beginner, I hope that these insights will inspire you to think more critically when playing your next grid!\nGoogle Colab Notebook ","permalink":"https://mindthegapblog.com/posts/wordle-solver-ai-cnn/","summary":"Are you a fan of Wordle, the addictive word puzzle game? Have you ever wondered what is the best word to play first? In this blog post, I\u0026rsquo;ll explore the use of Convolutional Neural Networks (CNNs) to solve Wordle grids with a success rate of over 80%.\nI should clarify that I\u0026rsquo;m not a data scientist – while I have a passion for machine learning and have completed a few courses on the subject, I am not an AI expert.","title":"Wordle-ing Our Way to Success: A CNN Experiment to Solve Wordle Grids"},{"content":"Hi everyone!\nI\u0026rsquo;m Valentin, an inventor, engineer, and product manager passionate about using technology to make the world a better place. In this blog, I plan to share my insights, experiences, and ideas on a wide range of topics, including technology, design, and society. My mission is to contribute to individual and collective intelligence, and to inspire and challenge my readers — you — to think critically and consider different perspectives.\nI grew up with a natural curiosity and a desire to solve problems with my ideas and inventions. This eventually led me to pursue a degree in Communication Systems and Computer Science at INSA Lyon. Today, I\u0026rsquo;m working as a Product Manager for Canonical, the company behind Ubuntu, where I\u0026rsquo;m contributing to bringing free software to the widest audience.\nOne of my earliest experiences with the Internet was as a teenager, when I actually didn\u0026rsquo;t have network access at home. I was determined to connect to the Internet, so I tried to remember and copy the IP and DNS configuration from the computers at school. Needless to say, my attempt was not successful! But looking back, I\u0026rsquo;m grateful that I didn\u0026rsquo;t have access to this powerful, unlimited resource too early in my life. It gave me a lot of time to be bored, which I believe has contributed to my creativity and thought-challenging attitude.\nNow, here I am writing to the world on the (real) Internet — this time! In addition to sharing my experiences and insights, I really hope to inspire some of you.\nBe sure to check out my upcoming blog entries on various topics such as using OKRs to create a healthy life schedule, managing your recurring tasks like a Sim, the benefits of open source (and not just in software), the potential of individual and collective intelligence, education\u0026hellip;\nThe format of this blog may evolve over time. Feel free to reach out to me on Twitter (you can find me at @ValentinViennot) or via email (blog@viennot.me) with any feedback or suggestions.\nThanks for reading, and I can\u0026rsquo;t wait to share more with you in the future!\nValentin.\n(DALL-E2 might be good at drawings, but not so much at writing texts.)\n","permalink":"https://mindthegapblog.com/posts/welcome-mindthegap-blog/","summary":"Hi everyone!\nI\u0026rsquo;m Valentin, an inventor, engineer, and product manager passionate about using technology to make the world a better place. In this blog, I plan to share my insights, experiences, and ideas on a wide range of topics, including technology, design, and society. My mission is to contribute to individual and collective intelligence, and to inspire and challenge my readers — you — to think critically and consider different perspectives.","title":"Welcome to the mind the gap blog!"}]